# 웹 크롤링
#### 파이썬으로 웹 페이지에 있는 정보를 가져오는 방법
1. 업로드해 둔 데이터를 다운로드 받기
2. API Server를 활용하기
3. <span style="color: red;">파이썬이 자동으로 검색 후 결과를 수집하기</span>
	- <span style="color: red;">크롤링(Crawling)</span>
    
### 웹 크롤링
- 여러 웹 페이지를 돌아다니며 원하는 정보를 모으는 기술이다.
- 원하는 정보를 추출하는 스크래핑(Scraping)과 여러 웹 페이지를 자동으로 탐색하는 크롤링(Crawling)의 개념을 합쳐 웹 크롤링이라고 한다.
- <span style="color: red;">필요한 데이터를 추출하여 활용할 수 있도록 자동화된 프로세스</span>이다.

#### 웹 크롤링 프로세스
- 웹 페이지 다운로드
  - 해당 웹 페이지의 HTML, CSS, JavaScript 등의 코드를 가져오는 단계
- 페이지 파싱
  - 다운로드 받은 코드를 분석하고 필요한 데이터를 추출하는 단계
- 링크 추출 및 다른 페이지 탐색
  - 다른 링크를 추출하고, 다음 단계로 이동하여 원하는 데이터를 추출하는 단계
- 데이터 추출 및 저장
  - 분석 및 시각화에 사용하기 위해 데이터를 처리하고 저장하는 단계

### 라이브러리
- <span style="color: red;">`requests`</span> : HTTP 요청을 보내고 응답을 받을 수 있는 모듈
- <span style="color: red;">`BeautifulSoup`</span> : HTML 문서에서 원하는 데이터를 추출하는 데 사용되는 파이썬 라이브러리
- <span style="color: red;">`Selenium`</span> : 웹 애플리케이션을 테스트하고 자동화하기 위한 파이썬 라이브러리
  - 웹 페이지의 동적인 컨텐츠를 가져오기 위해 사용한다.
  
```bash
$ pip install requests beautifulsoup4 selenium
```

#### `BeautifulSoup4` 주요 메서드
- `find()`
  - 태그를 사용하여 요소를 검색한다. 첫 번째로 일치하는 요소를 반환한다.
- `find_all()`
  - 태그를 사용하여 요소를 검색한다. 모든 일치하는 요소를 리스트로 반환한다.
- `select_one()`
  - CSS 선택자를 사용하여 요소를 검색한다. 첫 번째로 일치하는 요소를 반환한다.
- `select()`
  - CSS 선택자를 사용하여 요소를 검색한다. 모든 일치하는 요소를 리스트로 반환한다.
- `find_parent()`
  - 태그를 사용하여 요소를 검색한다. 각각 일치하는 요소의 부모 요소를 반환한다.
- `find_next_sibling()`
  - 태그를 사용하여 요소를 검색한다. 각각 일치하는 요소의 다음 형제 요소를 반환한다.
- `find_previous_sibling()`
  - 태그를 사용하여 요소를 검색한다. 각각 일치하는 요소의 이전 형제 요소를 반환한다.
```py
import requests
from bs4 import BeautifulSoup

url = 'https://quotes.toscrape.com/'

request = requests.get(url)
html_text = request.text
soup = BeautifulSoup(html_text, 'html.parser')

print(type(html_text))  # <class 'str'>
print(type(soup))  # <class 'bs4.BeautifulSoup'>

print(soup.find('a'))  # 해당 태그를 가진 첫 번째 요소를 검색
print(soup.find_all('a'))  # 해당 태그를 가진 모든 요소를 검색
print(soup.select_one('.text'))  # 해당 선택자를 가진 첫 번째 요소를 검색
print(soup.select('.text'))  # 해당 선택자를 가진 모든 요소를 검색
```
***

# 웹 크롤링 실습
### `selenium`
`requests` 모듈은 정적인 페이지만 요청한다. 따라서 동적인 페이지를 다루기 위해 `selenium` 모듈을 사용한다.
```py
from bs4 import BeautifulSoup
from selenium import webdriver


def get_google_data(keyword: str):
    url = f'https://www.google.com/search?q={keyword}'

    # 크롬 브라우저가 열린다.
    # 동적인 내용들이 모두 채워진다.
    driver = webdriver.Chrome()
    driver.get(url)

    # 열린 페이지 소스를 받아온다.
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    # 눈으로 보기 좋게 출력한다.
    print(soup.prettify())

    # 파일로 저장하여 확인한다.
    with open('soup.txt', 'w', encoding='utf-8') as file:
        file.write(soup.prettify())
```

### 태그 추출
```py
from bs4 import BeautifulSoup
from selenium import webdriver


def get_data(keyword: str):
    url = f'https://www.google.com/search?q={keyword}'

    # 크롬 브라우저가 열린다.
    # 동적인 내용들이 모두 채워진다.
    driver = webdriver.Chrome()
    driver.get(url)

    # 열린 페이지 소스를 받아온다.
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    # <div> 태그 중 id가 result-stats 인 요소를 검색한다.
    result_stats = soup.select_one('div#result-stats')
    print(result_stats.text)
```

웹 페이지를 개발자 도구로 열어 확인해보면 HTML, CSS, JavaScript 코드로 구성되어 있다. `id` 값은 새로고침마다 변하기 때문에, <span style="color: red;">`id` 값이 아닌 `class`와 태그를 기준으로 정보를 추출해야 한다.</span>
```py
from bs4 import BeautifulSoup
from selenium import webdriver


def get_google_data(keyword: str):
    url = f'https://www.google.com/search?q={keyword}'

    # 크롬 브라우저가 열린다.
    # 동적인 내용들이 모두 채워진다.
    driver = webdriver.Chrome()
    driver.get(url)

    # 열린 페이지 소스를 받아온다.
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    # div 태그 중 g 클래스를 가진 모든 요소를 선택한다.
    g_lst = soup.select('div.g')
    for g in g_lst:
        # 요소 안에 .LC20lb .MBeuO .DKV0Md 클래스를 가진 특정 요소를 선택한다.
        title = g.select_one('.LC20lb.MBeuO.DKV0Md')
        if title:
            print(f'제목 = {title.text}')
```
***

# Django에서 활용하기
Django에서 크롤링을 진행 후 데이터를 데이터베이스에 저장하고자 한다. 만약 게시글 제목과 검색어를 저장한다고 하면 `models.py`를 다음과 같이 설계한다.
```py
# models.py

class Article(models.Model):
	title = models.TextField()
    
class Query(models.Model):
	article = models.ForeignKey(Article, on_delete=models.DO_NOTHING)
	keyword = models.TextField()
```

> 정석이라면 게시글 제목과 검색어는 $N:M$의 관계를 갖는다.

```py
# views.py

def get_google_data(keyword: str) -> list:
	result = []
    
    url = f'https://www.google.com/search?q={keyword}'

    driver = webdriver.Chrome()
    driver.get(url)

    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')

    g_lst = soup.select('div.g')
    for g in g_lst:
        title = g.select_one('.LC20lb.MBeuO.DKV0Md')
        if title:
            result.append(title.text)


def crawling(request, keyword):
    titles = get_google_data(keyword)
    
    for title in titles:
    	article, created_article = Article.objects.get_or_create(title=title)
        query_obj, created_query = Query.objects.get_or_create(article=article, keyword=keyword)
```

> #### `get_or_create`
이미 있는 객체라면 가져오고 없으면 생성하는 QuerySet API이다. `created_` 변수에는 객체 생성 여부가 저장된다.
    	